[https://blog.csdn.net/dendi_hust/article/details/104465337](https://blog.csdn.net/dendi_hust/article/details/104465337)


  1. 什么是warmup
warmup是一种学习率优化方法（最早出现在ResNet论文中）。在模型训练之初选用较小的学习率，训练一段时间之后（如：10epoches或10000steps）使用预设的学习率进行训练；

2. 为什么使用warmup
2.1 理性分析

    因为模型的weights是随机初始化的，可以理解为训练之初模型对数据的“理解程度”为0（即：没有任何先验知识），在第一个epoches中，每个batch的数据对模型来说都是新的，模型会根据输入的数据进行快速调参，此时如果采用较大的学习率的话，有很大的可能使模型对于数据“过拟合”（“学偏”），后续需要更多的轮次才能“拉回来”；
    当模型训练一段时间之后（如：10epoches或10000steps），模型对数据具有一定的先验知识，此时使用较大的学习率模型就不容易学“偏”，可以使用较大的学习率加速模型收敛；
    当模型使用较大的学习率训练一段时间之后，模型的分布相对比较稳定，此时不宜从数据中再学到新特点，如果仍使用较大的学习率会破坏模型的稳定性，而使用小学习率更容易获取local optima。


# 我的总结：

训练初期损失较大，使用小的学习率，避免过拟合。

训练中期，有先验知识，学习率上升，加快收敛。

训练后期使用小学习率，稳定模型。